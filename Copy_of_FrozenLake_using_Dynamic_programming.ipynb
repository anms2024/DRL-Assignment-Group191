{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anms2024/DRL-Assignment-Group191/blob/create/Copy_of_FrozenLake_using_Dynamic_programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5BJ7jLz7afs"
      },
      "source": [
        "### Group ID:191\n",
        "### Group Members Name with Student ID:\n",
        "1. KAUSHAL RAJKOTIA - 2023ad05029@wilp.bits-pilani.ac.in\n",
        "2. M S ANJANA - 2023ac05498@wilp.bits-pilani.ac.in\n",
        "3. NAGENDRA KUMAR  - 2023ac05904@wilp.bits-pilani.ac.in\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCnLsZU8ObG6"
      },
      "source": [
        "1.**Problem statement**:\n",
        "\n",
        "* Develop a reinforcement learning agent using dynamic programming to solve the Treasure Hunt problem in a FrozenLake environment. The agent must learn the optimal policy for navigating the lake while avoiding holes and maximizing its treasure collection.\n",
        "\n",
        "2.**Scenario**:\n",
        "* A treasure hunter is navigating a slippery 5x5 FrozenLake grid. The objective is to navigate through the lake collecting treasures while avoiding holes and ultimately reaching the exit (goal).\n",
        "Grid positions on a 5x5 map with tiles labeled as S, F, H, G, T. The state includes the current position of the agent and whether treasures have been collected.\n",
        "\n",
        "\n",
        "#### Objective\n",
        "* The agent must learn the optimal policy π* using dynamic programming to maximize its cumulative reward while navigating the lake.\n",
        "\n",
        "#### About the environment\n",
        "\n",
        "The environment consists of several types of tiles:\n",
        "* Start (S): The initial position of the agent, safe to step.\n",
        "* Frozen Tiles (F): Frozen surface, safe to step.\n",
        "* Hole (H): Falling into a hole ends the game immediately (die, end).\n",
        "* Goal (G): Exit point; reaching here ends the game successfully (safe, end).\n",
        "* Treasure Tiles (T): Added to the environment. Stepping on these tiles awards +5 reward but does not end the game.\n",
        "\n",
        "After stepping on a treasure tile, it becomes a frozen tile (F).\n",
        "The agent earns rewards as follows:\n",
        "* Reaching the goal (G): +10 reward.\n",
        "* Falling into a hole (H): -10 reward.\n",
        "* Collecting a treasure (T): +5 reward.\n",
        "* Stepping on a frozen tile (F): 0 reward.\n",
        "\n",
        "#### States\n",
        "* Current position of the agent (row, column).\n",
        "* A boolean flag (or equivalent) for whether each treasure has been collected.\n",
        "\n",
        "#### Actions\n",
        "* Four possible moves: up, down, left, right\n",
        "\n",
        "#### Rewards\n",
        "* Goal (G): +10.\n",
        "* Treasure (T): +5 per treasure.\n",
        "* Hole (H): -10.\n",
        "* Frozen tiles (F): 0.\n",
        "\n",
        "#### Environment\n",
        "Modify the FrozenLake environment in OpenAI Gym to include treasures (T) at certain positions. Inherit the original FrozenLakeEnv and modify the reset and step methods accordingly.\n",
        "Example grid:\n",
        "\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2FN3CNXObG7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKsmJ9s4ObG7"
      },
      "source": [
        "**Expected Outcomes:**\n",
        "1.\tCreate the custom environment by modifying the existing “FrozenLakeNotSlippery-v0” in OpenAI Gym and Implement the dynamic programming using value iteration and policy improvement to learn the optimal policy for the Treasure Hunt problem.\n",
        "2.\tCalculate the state-value function (V*) for each state on the map after learning the optimal policy.\n",
        "3.\tCompare the agent’s performance with and without treasures, discussing the trade-offs in reward maximization.\n",
        "4.\tVisualize the agent’s direction on the map using the learned policy.\n",
        "5.\tCalculate expected total reward over multiple episodes to evaluate performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tPyiU_qObG7"
      },
      "source": [
        "### Import required libraries and Define the custom environment - 2 Marks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NI40i3dObG8"
      },
      "outputs": [],
      "source": [
        "# Import statements\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_uY1nRjObG9"
      },
      "outputs": [],
      "source": [
        "# Custom environment to create the given grid and respective functions that are required for the problem\n",
        "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
        "\n",
        "grid = [\n",
        "    \"SFFHT\",\n",
        "    \"FHFFF\",\n",
        "    \"FFFTF\",\n",
        "    \"TFHFF\",\n",
        "    \"FFFFG\"\n",
        "]\n",
        "class TreasureHuntFrozenLake(FrozenLakeEnv):\n",
        "    def __init__(self, desc=grid, map_name=\"5x5\", is_slippery=False): # fix: Renamed _init_ to __init__\n",
        "        super().__init__(desc=desc, map_name=map_name, is_slippery=is_slippery)\n",
        "        self.treasures = set()  # Store treasure positions\n",
        "\n",
        "    def reset(self):\n",
        "        obs = super().reset()\n",
        "        self.treasures = {(1, 2), (2, 1)}  # Example treasure positions\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        # Fix: Modified to handle the case when the parent class returns 3 values\n",
        "        # Assuming obs, reward, done,truncated, info are returned by the parent class's step method\n",
        "\n",
        "        step_result = super().step(action) # The parent step method might be returning 5 values\n",
        "        # Always unpack 5 values, as FrozenLakeEnv.step returns 5\n",
        "        obs, reward, done, truncated, info = step_result\n",
        "\n",
        "\n",
        "        row, col = divmod(self.s, self.ncol)\n",
        "\n",
        "        # Check for treasure\n",
        "        if (row, col) in self.treasures:\n",
        "            reward += 5\n",
        "            self.treasures.remove((row, col))  # Remove treasure\n",
        "            self.desc[row, col] = b'F'  # Change to frozen tile\n",
        "        return obs, reward, done, truncated, info # Always return 5 values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyb5DKQPObG9"
      },
      "source": [
        "### Value Iteration Algorithm - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-hg4wLxObG9"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, gamma=0.9, theta=1e-5):\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "    V = np.zeros(n_states)  # Initialize state-value function\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(n_states):\n",
        "            v = V[s]\n",
        "            q_values = []\n",
        "            for a in range(n_actions):\n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    q_value += prob * (reward + gamma * V[next_state] * (not done))\n",
        "                q_values.append(q_value)\n",
        "            V[s] = max(q_values)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Derive the optimal policy\n",
        "    policy = np.zeros([n_states, n_actions])\n",
        "    for s in range(n_states):\n",
        "        q_values = np.zeros(n_actions)\n",
        "        for a in range(n_actions):\n",
        "            for prob, next_state, reward, done in env.P[s][a]:\n",
        "                q_values[a] += prob * (reward + gamma * V[next_state] * (not done))\n",
        "        policy[s] = np.eye(n_actions)[np.argmax(q_values)]\n",
        "\n",
        "    return V, policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2eYM1RMObG9"
      },
      "source": [
        "### Policy Improvement Function - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MApo22z1ObG9"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(env, policy, V, gamma=0.9):\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "    policy_stable = True\n",
        "\n",
        "    for s in range(n_states):\n",
        "        old_action = np.argmax(policy[s])\n",
        "        q_values = np.zeros(n_actions)\n",
        "        for a in range(n_actions):\n",
        "            for prob, next_state, reward, done in env.P[s][a]:\n",
        "                q_values[a] += prob * (reward + gamma * V[next_state] * (not done))\n",
        "        new_action = np.argmax(q_values)\n",
        "        if new_action != old_action:\n",
        "            policy_stable = False\n",
        "        policy[s] = np.eye(n_actions)[new_action]\n",
        "    return policy, policy_stable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gR0Ngm8ObG9"
      },
      "source": [
        "### Print the Optimal Value Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMa7I2wtObG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800b3024-a5df-4f9a-831f-79c621b13b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            "[[0.4782969 0.531441  0.59049   0.        0.729    ]\n",
            " [0.531441  0.        0.6561    0.729     0.81     ]\n",
            " [0.59049   0.6561    0.729     0.81      0.9      ]\n",
            " [0.6561    0.729     0.        0.9       1.       ]\n",
            " [0.729     0.81      0.9       1.        0.       ]]\n"
          ]
        }
      ],
      "source": [
        "# Create the environment\n",
        "env = TreasureHuntFrozenLake()\n",
        "\n",
        "# Run value iteration\n",
        "V, policy = value_iteration(env)\n",
        "\n",
        "# Print the optimal value function\n",
        "print(\"Optimal Value Function:\")\n",
        "print(V.reshape(env.nrow, env.ncol))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTTfqePsObG-"
      },
      "source": [
        "### Visualization of the learned optimal policy - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfnTlfhwObG-"
      },
      "outputs": [],
      "source": [
        "# Policy visualization\n",
        "def visualize_policy(policy, env):\n",
        "    directions = ['←', '↓', '→', '↑']\n",
        "    grid = np.array(env.desc, dtype='str')\n",
        "    for s, actions in enumerate(policy):\n",
        "        row, col = divmod(s, env.ncol)\n",
        "        grid[row, col] = directions[np.argmax(actions)]\n",
        "    return \"\\n\".join(\"\".join(row) for row in grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT-JqNyLObG-"
      },
      "source": [
        "### Evaluate the policy - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLRF0xy2ObG-"
      },
      "outputs": [],
      "source": [
        "# Policy evaluation\n",
        "def evaluate_policy(env, policy, episodes=1000):\n",
        "    total_rewards = []\n",
        "    for _ in range(episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        rewards = 0\n",
        "        while not done:\n",
        "            action = np.argmax(policy[obs])\n",
        "            # Modified to unpack 5 values returned by env.step\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated # Set done if either terminated or truncated is True\n",
        "            rewards += reward\n",
        "        total_rewards.append(rewards)\n",
        "    return np.mean(total_rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sxQeuSHObG-"
      },
      "source": [
        "### Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pj1Eeg0ObG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ba43fe-6b3c-43a6-f9d7-be8338b9f4fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy with treasures:\n",
            " ↓→↓←↓\n",
            "↓←↓↓↓\n",
            "↓↓→↓↓\n",
            "↓↓←↓↓\n",
            "→→→→←\n",
            "\n",
            "Policy without treasures:\n",
            " ↓→↓←\n",
            "↓←↓←\n",
            "→↓↓←\n",
            "←→→←\n",
            "\n",
            "Expected total reward with treasures: 1.0\n",
            "Expected total reward without treasures: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Create environments\n",
        "env_with_treasures = TreasureHuntFrozenLake(map_name=\"4x4\") # Change map_name to \"4x4\" or \"8x8\"\n",
        "# Use '4x4' or '8x8' instead of '5x5'\n",
        "env_without_treasures = FrozenLakeEnv(map_name=\"4x4\", is_slippery=False)\n",
        "\n",
        "# Value iteration\n",
        "_, policy_with_treasures = value_iteration(env_with_treasures)\n",
        "_, policy_without_treasures = value_iteration(env_without_treasures)\n",
        "\n",
        "# Evaluate policies\n",
        "reward_with_treasures = evaluate_policy(env_with_treasures, policy_with_treasures)\n",
        "reward_without_treasures = evaluate_policy(env_without_treasures, policy_without_treasures)\n",
        "\n",
        "# Visualize policies\n",
        "policy_visual_with_treasures = visualize_policy(policy_with_treasures, env_with_treasures)\n",
        "policy_visual_without_treasures = visualize_policy(policy_without_treasures, env_without_treasures)\n",
        "\n",
        "# Output results\n",
        "print(\"Policy with treasures:\\n\", policy_visual_with_treasures)\n",
        "print(\"\\nPolicy without treasures:\\n\", policy_visual_without_treasures)\n",
        "print(\"\\nExpected total reward with treasures:\", reward_with_treasures)\n",
        "print(\"Expected total reward without treasures:\", reward_without_treasures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1zRujrfObG-"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}